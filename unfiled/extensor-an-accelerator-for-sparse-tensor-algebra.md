There's been a lot of work on matrix multiplication accelerators as a result of their applicability to machine learning. Matrix multiplication, however, is generalizable into tensor multiplication, which has applications in machine learning and many other scientific computing domains. Like matrix multiplications, many tensors are sparse.

This work focuses on the design and implementation of an accelerator that speeds up computation for sparse tensors. They rely on the fact that many operations can be skipped due to the sparsity of the input tensors. AFAICT, they leverage specific details of the compression format for the input tensors to skip large parts of the computation. They feed the input tensors in as streams and have an accelerator which automatically determines which parts of the computation can be skipped as the tensor comes in as a stream.

One part of the paper that I didn't get: I was expecting a speedup of tens of x over a CPU. In section 8, their given speedup seems pretty modest. I'm sure that they explained why this is & I just missed it cause I was giving the paper a quick read.... maybe DRAM bandwidth, maybe compression format, maybe sparsity of specific input tensors.