RAM takes up a nontrivial amount of power in systems that evaluate DNNs; paragraph 3 of this paper's introduction says that DRAM takes up to 80% of the system's energy in some DNN evaluation systems. The authors propose a method for making DRAM take less power by undervolting it and violating its timing parameters - all things which can be done by commodity motherboards and memory controllers. Even though DNNs are well known for their error-resilency properties, some bit-errors are more lethal than others: an error in the exponent can kill a calculation whereas an error in the LSBit of the mantissa is tolerable. The authors give 2 mechanisms for dealing with this. First, they have a modified training technique where the network is first trained normally, then DRAM errors are slowly introduced. If bit errors are introduced too early, then the network won't train at all; if they're introduced too late, the network won't be able to adapt. Also, the authors have a heuristic mechanism where they reject "implausible" values, going off of the observation that many networks have their weights constrained to a specific range. I take issue with this idea; if a network's values are constrained to [-5, 5] (an example they give), then one should just alter the data format to save many bytes. You could "cut off the unused range" and add an error detection bit at less of a memory cost.
