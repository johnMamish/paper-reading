It's well known that there's a lot of redundancy in deep neural networks, and that by avoiding some of this redundancy, there is low-hanging fruit for performance and energy savings in DNN evaluation. Taking advantage of the observation that there are weights within a DNN that are non-contributing from early-on in training, they propose "Eager Pruning", where weights that don't change much during training are tossed out early. This provides some opportunities for architectural innovations; the sparsity of the network will change as training progresses (discussed in section 4 "Structure of DRACT"), so special architectural considerations are needed to bypass / reschedule pruned weights as efficiently as possible.