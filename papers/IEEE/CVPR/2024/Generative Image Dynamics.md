## Generative Image Dynamics
#### Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski, 2024
#### Summary
Animating motion of mostly-still scenes whose dynamics are simple by governed by complicated underlying processes (e.g. flowers swaying in a field, candles burning on a cake) is of interest to artistic work like animation. This paper proposes a method for generating motion fields from still images, which can then be used to produce short animation loops (examples at generative-dynamics.github.io). Previous work attempts to do this by using text-to-video or directly outputting video from images, which can be difficult to control or result in odd artifacts; by predicting optical flow, which only gives instantaneous pixel velocities (although the authors don't seem to elaborate on why this is an issue); or by using 'video textures', which require full videos of input, not single images.

Unlike previous works, the authors represent scene motion with spectral models in the time-domain, which is well-suited to looping motion. They train a network which takes an input video loop and outputs a spectral volume - a sequence of fourier-domain frames which have x- and y- frequencies and phases for each pixel. This network is then fed into a gaussing splatting network which generates output frames from a single input frames and the frequency-domain information, thereby dealing with occulded pixels from motion. They train these two networks back-to-back to generate output video that mimics input video. New video is generated by diffusing spectral volumes out of a never-before-seen input frame and then splatting the input frame using the spectral volumes.

They evaluate their work by generating video from the first frame of a ground-truth video and comparing it to the ground-truth video. They also qualitatively look at generated videos. They also conduct an ablation study to look at design-space constraints.

I think it would be interesting to see what future directions lay in integrating this work with text-to-video models. Perhaps including spectral information can make generation of video more tractable.

#### Strengths
  - Method uses insight into natural processes to better model them.
  - Proposed method requires less compute than a purely time-domain method.
  - Excellent results.
  - Great demo website ([generative-dynamics.github.io](generative-dynamics.github.io))

#### Weaknesses
  - Would've liked to see a direct comparison with time-domain methods
  - Not much discussion about potential applications. I can imagine some applications (computer animation, video games especially), but some discussion about how to apply this method to those would be cool!

#### Additional comments
Perhaps there's not much discussion about applications due to IP concerns

#### What potential directions of future work remain (if any)?
Applications!! lol

Keywords: [[deep-learning]], [[keywords/diffusion-model]]
Tags:
Read date: [[read date/2024/july]]